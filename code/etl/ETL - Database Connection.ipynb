{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449d0ad3",
   "metadata": {},
   "source": [
    "# ETL - Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbea21",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b700915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sqlalchemy in /opt/tljh/user/lib/python3.9/site-packages (1.4.29)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from sqlalchemy) (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28891462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymysql in /home/jupyter-svanhemert00/.local/lib/python3.9/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8832645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /home/jupyter-svanhemert00/.local/lib/python3.9/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ed8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Library imports\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2bf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set pandas settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aad873",
   "metadata": {},
   "source": [
    "## Replicate Data Warehouse Schema from `final_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359e3220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DataFrame from CSV file: data/bulk_data_20240410_083343.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to the 'data' subfolder\n",
    "data_folder = 'data'\n",
    "\n",
    "# Get a list of all CSV files in the 'data' subfolder\n",
    "csv_files = [os.path.join(data_folder, file) for file in os.listdir(data_folder) if file.endswith('.csv') and file.startswith('bulk_data_')]\n",
    "\n",
    "# Check if any CSV files exist in the 'data' subfolder\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV files found in the 'data' subfolder with the specified prefix.\")\n",
    "\n",
    "# Sort CSV files by modification time (most recent first)\n",
    "csv_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "# Select the most recent CSV file\n",
    "latest_csv_file = csv_files[0]\n",
    "\n",
    "# Read in the most recent CSV file\n",
    "final_df = pd.read_csv(latest_csv_file)\n",
    "\n",
    "# Print the name of the file being read\n",
    "print(f\"Reading DataFrame from CSV file: {latest_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8e0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dimension tables\n",
    "# dim_chart_month = final_df[['chart_month', 'track_id']].drop_duplicates().reset_index(drop=True)\n",
    "dim_album = final_df[['album', 'main_artist', 'album_type']].drop_duplicates().reset_index(drop=True)\n",
    "dim_pitch_class = final_df[['pitch_key', 'tonal_counterparts', 'solfege']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "## Create dim_artist\n",
    "max_followers = final_df.groupby('main_artist')['followers'].idxmax()\n",
    "dim_artist = final_df.loc[max_followers][[\n",
    "    'main_artist', 'followers']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "## Create dim_song\n",
    "max_weeks_index = final_df.groupby('track_id')['weeks_on_chart'].idxmax()\n",
    "dim_song = final_df.loc[max_weeks_index][[\n",
    "    'track_id', 'track_name', 'all_artists', 'album', 'release_date',\n",
    "    'weeks_on_chart', 'preview_url', 'cover_art_url', 'external_url',\n",
    "    'analysis_url', 'explicit', 'track_number', 'available_markets',\n",
    "    'uri', 'track_href'\n",
    "]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "## Create dim_genre\n",
    "dim_genre_data = []\n",
    "for index, row in final_df.iterrows():\n",
    "    main_artist = row['main_artist']\n",
    "    genres_list_str = row['genres']\n",
    "    # convert the string representation of list into a list object\n",
    "    genres_list = ast.literal_eval(genres_list_str)\n",
    "    for genre in genres_list:\n",
    "        dim_genre_data.append({'main_artist': main_artist, 'genre': genre})\n",
    "dim_genres = pd.DataFrame(dim_genre_data).drop_duplicates()\n",
    "\n",
    "### Create fact table\n",
    "avg_popularity_index = final_df.groupby('track_id')['popularity'].idxavg()\n",
    "fact_table = final_df[[\n",
    "    'track_id', 'main_artist', avg_popularity_index, 'danceability', 'energy', 'pitch_key',\n",
    "    'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "    'liveness', 'valence', 'tempo', 'time_signature', 'duration_sec'\n",
    "]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40099da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify df column names and datatypes match that of database schema:\n",
      "\n",
      "dim_album\n",
      "album          object\n",
      "main_artist    object\n",
      "album_type     object\n",
      "dtype: object\n",
      "\n",
      "dim_artist\n",
      "main_artist    object\n",
      "followers       int64\n",
      "dtype: object\n",
      "\n",
      "dim_pitch_class\n",
      "pitch_key              int64\n",
      "tonal_counterparts    object\n",
      "solfege               object\n",
      "dtype: object\n",
      "\n",
      "dim_song\n",
      "track_id             object\n",
      "track_name           object\n",
      "all_artists          object\n",
      "album                object\n",
      "release_date         object\n",
      "popularity            int64\n",
      "weeks_on_chart        int64\n",
      "preview_url          object\n",
      "cover_art_url        object\n",
      "external_url         object\n",
      "analysis_url         object\n",
      "explicit               bool\n",
      "track_number          int64\n",
      "available_markets    object\n",
      "uri                  object\n",
      "track_href           object\n",
      "dtype: object\n",
      "\n",
      "dim_genres\n",
      "main_artist    object\n",
      "genre          object\n",
      "dtype: object\n",
      "\n",
      "fact_table\n",
      "track_id             object\n",
      "main_artist          object\n",
      "danceability        float64\n",
      "energy              float64\n",
      "pitch_key             int64\n",
      "loudness            float64\n",
      "mode                 object\n",
      "speechiness         float64\n",
      "acousticness        float64\n",
      "instrumentalness    float64\n",
      "liveness            float64\n",
      "valence             float64\n",
      "tempo               float64\n",
      "time_signature        int64\n",
      "duration_sec        float64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Verify df column names and datatypes match that of database schema\n",
    "print('Verify df column names and datatypes match that of database schema:')\n",
    "print()\n",
    "print('dim_album')\n",
    "print(dim_album.dtypes)\n",
    "print()\n",
    "print('dim_artist')\n",
    "print(dim_artist.dtypes)\n",
    "print()\n",
    "print('dim_pitch_class')\n",
    "print(dim_pitch_class.dtypes)\n",
    "print()\n",
    "print('dim_song')\n",
    "print(dim_song.dtypes)\n",
    "print()\n",
    "print('dim_genres')\n",
    "print(dim_genres.dtypes)\n",
    "print()\n",
    "print('fact_table')\n",
    "print(fact_table.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023fb1a",
   "metadata": {},
   "source": [
    "## Ingestion to Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08bdba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked fine breh\n"
     ]
    }
   ],
   "source": [
    "### Data Ingestion\n",
    "\n",
    "## Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "## 1. Connect to your RDS database\n",
    "# Replace 'username', 'password', 'host', 'port', and 'database_name' with your credentials\n",
    "\n",
    "username = os.environ.get('RDS_USERNAME')\n",
    "password = os.environ.get('RDS_PASSWORD')\n",
    "host = os.environ.get('RDS_HOST')\n",
    "port = os.environ.get('RDS_PORT')\n",
    "database_name = os.environ.get('RDS_DATABASE')\n",
    "\n",
    "## 2. Create SQLAlchemy engine for MySQL\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{host}:{port}/{database_name}\")\n",
    "\n",
    "## 3. Read your Pandas DataFrame (assuming you already have it loaded)\n",
    "# Replace 'df' with the name of your DataFrame\n",
    "# Replace 'table_name' with the name of the table you want to create in the database\n",
    "# Replace 'if_exists' parameter with 'replace' if you want to replace the table if it already exists\n",
    "dim_artist.to_sql('dim_artist', engine, if_exists='append', index=False)\n",
    "dim_album.to_sql('dim_album', engine, if_exists='append', index=False)\n",
    "dim_genres.to_sql('dim_genres', engine, if_exists='append', index=False)\n",
    "dim_pitch_class.to_sql('dim_pitch_class', engine, if_exists='append', index=False)\n",
    "dim_song.to_sql('dim_song', engine, if_exists='append', index=False)\n",
    "fact_table.to_sql('fact_table', engine, if_exists='append', index=False)\n",
    "\n",
    "## Close the engine connection\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
